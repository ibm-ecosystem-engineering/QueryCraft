[DataIngestion]
# Enter the values for you database connection. This can be found in DB2's Service Credentials from the tooling. 
dsn_database = BLUDB
dsn_uid      = bluadmin
dsn_pwd      = E12USuJ@MPnPHSq8ZKJ2pQON0GQzL
dsn_hostname = db2w-dimxrms.us-south.db2w.cloud.ibm.com
dsn_port     = 50001
#dsn_protocol = TCPIP
dsn_protocol= SSL
dsn_driver   = IBM DB2 ODBC DRIVER
#delimiter of  input file
delimiter = ,


[Finetune]
precision = 8
#Max context length
tokenizeMaxLength = 512
#LoRA hyperparameters rank, alpha and dropout
LoRA_r = 64
LoRA_alpha = 32
LoRA_dropout=0.1
#Batch Size
batch_size = 16
#In case of gradient accumulation, per device train batch size
per_device_train_batch_size = 1
#Select target modules for LoRA, either attention_linear_layers or all_linear_layers
target_modules = attention_linear_layers
#Number of epochs
num_train_epochs = 1

[Inference] 
#Watsonx credentials
watsonx_url = https://us-south.ml.cloud.ibm.com
watsonx_apikey = 
watsonx_projectID = 


[logs]
#Path to logs folder, all log files would be created in this path
log_folder = output/logs/


[EXEvaluator]


[QueryCorrection]
# The query_correction attribute is set to either 0 or 1, determining whether this module will be executed. By default, it's set to 1; if changed to 0, it indicates that the module will not be activated.
query_correction = 1



[QueryAnalysisDashboard]
text2sql_exp_file = output/result/text2sql_exp_results.csv
# The output file of token length which uses some examples of the open-source dataset
token_data_file = input/datasets/token_len_SQLModel.csv
# This image shows the spider benchmark which is available at their site
benchmark_image = output/benchmark/spider_benchmark.png
# combination of open source dataset file
input_dataset_file = input/datasets/spiderTrain_BIRDTrain_BIRDDev_CoSQLTran_CoSQLDev_SparcTrain_SparcDev_KaggleDBQA_withSource.csv
# Selected columns to create the CSV file for all experiments
selected_columns = Base_Model, Evaluation_set, Ex-accuracy, PP-Ex-accuracy, R, precision, Training_Set, LORA_Alpha, LORA_Dropout, Finetune_Strategy, Target_Modules, Task_Type, Epoch, Learning_Rate, Loss, Eval_Loss, Eval_Runtime, Eval Samples/Second, Eval Steps/Second, Logging_Steps, Max_Steps
