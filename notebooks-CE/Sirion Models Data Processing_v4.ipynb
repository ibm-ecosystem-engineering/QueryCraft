{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e79223c-4021-40a9-87c5-448a753502a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/askyourcorpus/aditya_qc_experiments/QueryCraft-fork/output/evalResultsDB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6c4a45-a73a-4f1c-8900-99a2f6b686f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "from sql_metadata import Parser\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import logging\n",
    "import ibm_db\n",
    "\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from datetime import datetime\n",
    "import os\n",
    "from math import ceil\n",
    "import random\n",
    "import sys\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e2d87-029d-4bd4-8e1c-8420ab676ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_keywords_list = [\n",
    "    \"TOP\",\"EXISTS\",\"INTERSECT\",\"SELECT\",\"DISTINCT\",\"RANK\",\"AS\",\n",
    "    \"WHERE\",\"AND\",\"OR\",\"BETWEEN\",\"LIKE\",\"COUNT\",\"SUM\",\"AVG\",\n",
    "    \"MIN\",\"MAX\",\"GROUP BY\",\"ORDER BY\",\"DESC\",\"OFFSET\",\"FETCH\",\n",
    "    \"INNER JOIN\",\"LEFT JOIN\",\"RIGHT JOIN\",\"FULL JOIN\",\"UNION\",\n",
    "    \"HAVING\",\"JOIN\"\n",
    "]\n",
    "aggregate_keywords = [\"COUNT\",\"SUM\",\"AVG\",\"MIN\",\"MAX\",\"TOP\"]\n",
    "rank_keywords = [\"RANK\"]\n",
    "fillter_keywords = [\"GROUP BY\",\"ORDER BY\",\"FILTER\",\"HAVING\",\"EXISTS\"]\n",
    "join_keywords = [\n",
    "    \"JOIN\",\"INNER JOIN\",\"LEFT JOIN\",\"RIGHT JOIN\",\"FULL JOIN\",\"UNION\",\"INTERSECT\"\n",
    "]\n",
    "orderby_keywords = [\"ORDER BY\"]\n",
    "groupby_keywords = [\"GROUP BY\"]\n",
    "where_keywords =[\"WHERE\"]\n",
    "date_keywords =[\n",
    "    \"NOW\",\"GETDATE\",\"CURRENT_TIMESTAMP\",\"DATEDIFF\",\"DATEADD\",\"YEAR\",\"DAY\",\"MONTH\"\n",
    "]\n",
    "keyword_pattern = re.compile(r'\\b(?:' + '|'.join(query_keywords_list) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "def calculate_classification_new(df):\n",
    "    ## Create new 3 columns \n",
    "    df[\"Expected count\"] = df.index\n",
    "    df[\"Expected difficulty\"] = df.index\n",
    "    df[\"Expected classification_new\"] = df.index\n",
    "    for index, row in df.iterrows():\n",
    "        sql = row[\"expected query\"]\n",
    "        count =0\n",
    "        classification =\"\"\n",
    "        for keyword in query_keywords_list:\n",
    "            if keyword in sql:\n",
    "                count=count+1\n",
    "                if keyword in orderby_keywords:\n",
    "                    classification = \"ORDER BY\"\n",
    "                elif keyword in groupby_keywords:\n",
    "                    classification = \"GROUP BY\"\n",
    "                elif keyword in aggregate_keywords:\n",
    "                    classification = \"AGGREGATE/RATIO\"\n",
    "                elif keyword in join_keywords:\n",
    "                    classification = \"JOIN\"\n",
    "                elif keyword in where_keywords:\n",
    "                    classification = \"WHERE\"\n",
    "                elif keyword in date_keywords:\n",
    "                    classification = \"DATE\"\n",
    "                ## join  for category \n",
    "                ## pre-trained codellama model what type of query are not correct.\n",
    "\n",
    "        if count < 6:\n",
    "            df.at[index,'Expected difficulty'] =\"simple\"\n",
    "        elif count > 5 and count < 9:\n",
    "            df.at[index,'Expected difficulty'] =\"moderate\"\n",
    "        else:\n",
    "            df.at[index,'Expected difficulty'] =\"challenging\"\n",
    "        if classification == '':\n",
    "            classification = 'SELECT'\n",
    "\n",
    "        df.at[index,'Expected classification_new'] = classification\n",
    "        df.at[index,'Expected count'] =count\n",
    "\n",
    "\n",
    "    ## Create new 3 columns \n",
    "    df[\"Predicted count\"] = df.index\n",
    "    df[\"Predicted difficulty\"] = df.index\n",
    "    df[\"Predicted classification_new\"] = df.index\n",
    "    for index, row in df.iterrows():\n",
    "        sql = row[\"generated query\"]\n",
    "        count =0\n",
    "        classification =\"\"\n",
    "        for keyword in query_keywords_list:\n",
    "            if keyword in sql:\n",
    "                count=count+1\n",
    "                if keyword in orderby_keywords:\n",
    "                    classification = \"ORDER BY\"\n",
    "                elif keyword in groupby_keywords:\n",
    "                    classification = \"GROUP BY\"\n",
    "                elif keyword in aggregate_keywords:\n",
    "                    classification = \"AGGREGATE/RATIO\"\n",
    "                elif keyword in join_keywords:\n",
    "                    classification = \"JOIN\"\n",
    "                elif keyword in where_keywords:\n",
    "                    classification = \"WHERE\"\n",
    "                elif keyword in date_keywords:\n",
    "                    classification = \"DATE\"\n",
    "                ## join  for category \n",
    "                ## pre-trained codellama model what type of query are not correct.\n",
    "\n",
    "        if count < 6:\n",
    "            df.at[index,'Predicted difficulty'] =\"simple\"\n",
    "        elif count > 5 and count < 9:\n",
    "            df.at[index,'Predicted difficulty'] =\"moderate\"\n",
    "        else:\n",
    "            df.at[index,'Predicted difficulty'] =\"challenging\"\n",
    "        if classification == '':\n",
    "            classification = 'SELECT'\n",
    "\n",
    "        df.at[index,'Predicted classification_new'] = classification\n",
    "        df.at[index,'Predicted count'] =count\n",
    "    return df\n",
    "\n",
    "def calculate_classification(df):\n",
    "    ## Create new 3 columns \n",
    "    df[\"Expected count\"] = df.index\n",
    "    df[\"Expected difficulty\"] = df.index\n",
    "    df[\"Expected classification\"] = df.index\n",
    "    for index, row in df.iterrows():\n",
    "        sql = row[\"expected query\"]\n",
    "        count =0\n",
    "        classification =\"\"\n",
    "        for keyword in query_keywords_list:\n",
    "            if keyword in sql:\n",
    "                count=count+1\n",
    "                if keyword in rank_keywords:\n",
    "                    classification = \"RANK\"\n",
    "                elif keyword in fillter_keywords:\n",
    "                    classification = \"FILTER\"\n",
    "                elif keyword in aggregate_keywords:\n",
    "                    classification = \"AGGREGATE\"\n",
    "                elif keyword in join_keywords:\n",
    "                    classification = \"JOIN\"\n",
    "                ## join  for category \n",
    "                ## pre-trained codellama model what type of query are not correct.\n",
    "\n",
    "        if count < 6:\n",
    "            df.at[index,'Expected difficulty'] =\"simple\"\n",
    "        elif count > 5 and count < 9:\n",
    "            df.at[index,'Expected difficulty'] =\"moderate\"\n",
    "        else:\n",
    "            df.at[index,'Expected difficulty'] =\"challenging\"\n",
    "        if classification == '':\n",
    "            classification = 'SELECT'\n",
    "\n",
    "        df.at[index,'Expected classification'] = classification\n",
    "        df.at[index,'Expected count'] =count\n",
    "\n",
    "        ## Create new 3 columns \n",
    "    df[\"Predicted count\"] = df.index\n",
    "    df[\"Predicted difficulty\"] = df.index\n",
    "    df[\"Predicted classification\"] = df.index\n",
    "    for index, row in df.iterrows():\n",
    "        sql = row[\"generated query\"]\n",
    "        count =0\n",
    "        classification =\"\"\n",
    "        for keyword in query_keywords_list:\n",
    "            if keyword in sql:\n",
    "                count=count+1\n",
    "                if keyword in rank_keywords:\n",
    "                    classification = \"RANK\"\n",
    "                elif keyword in fillter_keywords:\n",
    "                    classification = \"FILTER\"\n",
    "                elif keyword in aggregate_keywords:\n",
    "                    classification = \"AGGREGATE\"\n",
    "                elif keyword in join_keywords:\n",
    "                    classification = \"JOIN\"\n",
    "                ## join  for category \n",
    "                ## pre-trained codellama model what type of query are not correct.\n",
    "\n",
    "        if count < 6:\n",
    "            df.at[index,'Predicted difficulty'] =\"simple\"\n",
    "        elif count > 5 and count < 9:\n",
    "            df.at[index,'Predicted difficulty'] =\"moderate\"\n",
    "        else:\n",
    "            df.at[index,'Predicted difficulty'] =\"challenging\"\n",
    "        if classification == '':\n",
    "            classification = 'SELECT'\n",
    "\n",
    "        df.at[index,'Predicted classification'] = classification\n",
    "        df.at[index,'Predicted count'] =count\n",
    "    return df\n",
    "\n",
    "def tokenize_sql_query(sql_query):\n",
    "    token_list = []\n",
    "    try:\n",
    "        for token in Parser(sql_query).tokens:\n",
    "            token_list.append(str(token.value))\n",
    "    except:\n",
    "        pass\n",
    "    return token_list\n",
    "\n",
    "\n",
    "\n",
    "def get_category(query):\n",
    "    query_keywords_list = [\"TOP\",\"EXISTS\",\"INTERSECT\",\"SELECT\",\"DISTINCT\",\"TOP\",\"RANK\",\"AS\",\"WHERE\",\"AND\",\"OR\",\"BETWEEN\",\"LIKE\",\"COUNT\",\"SUM\",\"AVG\",\"MIN\",\"MAX\",\"GROUP BY\",\"ORDER BY\",\"DESC\",\"OFFSET\",\"FETCH\",\"INNER JOIN\",\"LEFT JOIN\",\"RIGHT JOIN\",\"FULL JOIN\",\"UNION\",\"HAVING\",\"JOIN\"]\n",
    "    aggregate_keywords = [\"COUNT\",\"SUM\",\"AVG\",\"MIN\",\"MAX\",\"TOP\"]\n",
    "    rank_keywords = [\"RANK\"]\n",
    "    fillter_keywords = [\"GROUP BY\",\"ORDER BY\",\"FILTER\",\"HAVING\",\"EXISTS\"]\n",
    "    join_keywords = [\"JOIN\",\"INNER JOIN\",\"LEFT JOIN\",\"RIGHT JOIN\",\"FULL JOIN\",\"UNION\",\"INTERSECT\"]\n",
    "\n",
    "    count =0\n",
    "    orderby_keywords = [\"ORDER BY\"]\n",
    "    groupby_keywords = [\"GROUP BY\"]\n",
    "    where_keywords =[\"WHERE\"]\n",
    "    date_keywords =[\"NOW\",\"GETDATE\",\"CURRENT_TIMESTAMP\",\"DATEDIFF\",\"DATEADD\",\"YEAR\",\"DAY\",\"MONTH\"]\n",
    "    for keyword in query_keywords_list:\n",
    "        if isinstance(query,str) and keyword in query:\n",
    "            count=count+1\n",
    "            if keyword in orderby_keywords:\n",
    "                classification = \"ORDER BY\"\n",
    "            elif keyword in groupby_keywords:\n",
    "                classification = \"GROUP BY\"\n",
    "            elif keyword in aggregate_keywords:\n",
    "                classification = \"AGGREGATE/RATIO\"\n",
    "            elif keyword in join_keywords:\n",
    "                classification = \"JOIN\"\n",
    "            elif keyword in where_keywords:\n",
    "                classification = \"WHERE\"\n",
    "            elif keyword in date_keywords:\n",
    "                classification = \"DATE\"\n",
    "    if count < 6:\n",
    "        classification=\"simple\"\n",
    "    elif count > 5 and count < 9:\n",
    "        classification=\"moderate\"\n",
    "    else:\n",
    "        classification =\"challenging\"\n",
    "    if classification == '':\n",
    "        classification = 'SELECT'\n",
    "    return classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5884dc4e-634b-4afb-afae-d3ca10052a99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_path = \"../output/evalResults_qc/\"\n",
    "os.makedirs(\"../output/inference_\",exist_ok=True)\n",
    "os.makedirs(\"../output/inference_/accumulated_accuracy\",exist_ok=True)\n",
    "model_paths = os.listdir(base_path)\n",
    "\n",
    "\n",
    "dict_accuracy_overall = {}\n",
    "dict_latency_overall = {}\n",
    "\n",
    "\n",
    "for model_path in model_paths:\n",
    "    \n",
    "    df = pd.read_csv(base_path+model_path)\n",
    "    model_id = model_path.split(\".csv\")[0]\n",
    "\n",
    "    \n",
    "    df[\"predicted_query_toks\"] = df[\"generated query\"].apply(tokenize_sql_query).astype(str).replace(\",\",\" \")\n",
    "    df[\"expected_query_toks\"] = df[\"expected query\"].apply(tokenize_sql_query).astype(str).replace(\",\",\" \")\n",
    "    \n",
    "    df['expected_sql_classification'] = df['expected query'].apply(lambda sql: get_category(sql))\n",
    "    df['generated_sql_classification'] = df['generated query'].apply(lambda sql: get_category(sql))\n",
    "    df = calculate_classification(df,)\n",
    "    df = calculate_classification_new(df)\n",
    "    df.to_csv(\"../output/inference_/{}.csv\".format(model_id),index=False)\n",
    "\n",
    "    filtered_df = df\n",
    "    grouped = df.groupby('generated_sql_classification')\n",
    "    execution_accuracy = {}\n",
    "    \n",
    "    cat_df = pd.DataFrame(columns=[\"category\",\"Total evaluation records for category\",\"Total correct responses\",\"Execution accuracy %\"])\n",
    "    for name, group in grouped:\n",
    "        cat_list=[]\n",
    "        total_true = group['evalScore'].sum()\n",
    "        total_records = len(group)\n",
    "        print(total_true,total_records,\"##\")\n",
    "        accuracy = total_true / total_records\n",
    "        execution_accuracy[name] = accuracy\n",
    "        cat_list.append(name)\n",
    "        cat_list.append(total_records)\n",
    "        cat_list.append(total_true)\n",
    "        cat_list.append(accuracy)\n",
    "        print(cat_list)\n",
    "        new_row_df = pd.DataFrame([cat_list], columns=cat_df.columns)\n",
    "        cat_df = pd.concat([cat_df, new_row_df], ignore_index=True)\n",
    "    \n",
    "    \n",
    "    tot_list = []\n",
    "    category = \"TOTAL\"\n",
    "    total_eval_records = filtered_df.shape[0]\n",
    "    evalScore_counts = df[df['evalScore'] == True]['evalScore'].value_counts()[0]\n",
    "    exec_accuracy =  evalScore_counts/total_eval_records\n",
    "    tot_list.append(category)\n",
    "    tot_list.append(total_eval_records)\n",
    "    tot_list.append(evalScore_counts)\n",
    "    tot_list.append(exec_accuracy)\n",
    "    new_row_df = pd.DataFrame([tot_list], columns=cat_df.columns)\n",
    "    cat_df = pd.concat([cat_df, new_row_df], ignore_index=True)\n",
    "    cat_df['Execution accuracy %'] =round(cat_df['Execution accuracy %']*100,2)\n",
    "    cat_df['Latency'] = round(df['latency'].mean(),3)\n",
    "    \n",
    "    cat_df.to_csv(\"../output/inference_/accumulated_accuracy/{}.csv\".format(model_id),index=False)\n",
    "    dict_accuracy_overall[model_id] =  cat_df[cat_df[\"category\"]==\"TOTAL\"][\"Execution accuracy %\"].values[0]\n",
    "    dict_latency_overall[model_id] = round(df['latency'].mean(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb578ead-7352-4647-ada7-78bc14f4bcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_accuracy_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dec295-4b5e-45ef-8cc8-1a32a4ad1f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_latency_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9173eb5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1043c03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "base_path_inference = \"../output/inference_/\"\n",
    "df_selected = pd.read_csv(base_path_inference+model_path)\n",
    "\n",
    "# filtered_df = df_selected[df_selected['expected_sql_valid']==True]\n",
    "grouped = df_selected.groupby('Expected classification_new')\n",
    "\n",
    "execution_accuracy = {}\n",
    "\n",
    "cat_df = pd.DataFrame(columns=[\"category\",\"Total evaluation records for category\",\"Total correct responses\",\"Execution accuracy %\"])\n",
    "for name, group in grouped:\n",
    "    cat_list=[]\n",
    "    total_true = group['evalScorePostProcessing'].sum()\n",
    "    total_records = len(group)\n",
    "    #print(total_true,total_records,\"##\")\n",
    "    accuracy = total_true / total_records\n",
    "    execution_accuracy[name] = accuracy\n",
    "    cat_list.append(name)\n",
    "    cat_list.append(total_records)\n",
    "    cat_list.append(total_true)\n",
    "    cat_list.append(accuracy)\n",
    "    #print(cat_list)\n",
    "    new_row_df = pd.DataFrame([cat_list], columns=cat_df.columns)\n",
    "    cat_df = pd.concat([cat_df, new_row_df], ignore_index=True)\n",
    "\n",
    "total_aggregate = cat_df[cat_df[\"category\"] ==  \"TOTAL\"]\n",
    "\n",
    "total_evaluation_records = cat_df['Total evaluation records for category'].sum()\n",
    "cat_df['Weights'] = cat_df['Total evaluation records for category'] / total_evaluation_records\n",
    "\n",
    "tot_list = []\n",
    "category = \"TOTAL\"\n",
    "total_eval_records = df_selected.shape[0]\n",
    "evalScore_counts = df_selected[df_selected['evalScorePostProcessing'] == True]['evalScorePostProcessing'].value_counts()[0]\n",
    "exec_accuracy =  evalScore_counts/total_eval_records*100\n",
    "tot_list.append(category)\n",
    "tot_list.append(total_eval_records)\n",
    "tot_list.append(evalScore_counts)\n",
    "tot_list.append(exec_accuracy)\n",
    "cat_df['Execution accuracy %'] = round(cat_df['Execution accuracy %']*100,2)\n",
    "cat_df['Weighted execution accuracy'] = cat_df['Execution accuracy %'] * cat_df['Weights']\n",
    "\n",
    "tot_list.append(np.nan)\n",
    "tot_list.append(np.nan)\n",
    "new_row_df = pd.DataFrame([tot_list], columns=cat_df.columns)\n",
    "cat_df = pd.concat([cat_df, new_row_df], ignore_index=True)\n",
    "cat_df['Average Latency'] = np.nan\n",
    "cat_df['Average Latency'].iloc[-1] = round(df_selected['latency'].mean(),3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5042558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(cat_df[\"category\"],cat_df['Execution accuracy %']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332e4a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGGREGATE_RATIO = []\n",
    "GROUP_BY = []\n",
    "ORDER_BY = []\n",
    "SELECT =[]\n",
    "WHERE = []\n",
    "TOTAL_AVERAGE = []\n",
    "\n",
    "AGGREGATE_RATIO_COUNT = []\n",
    "GROUP_BY_COUNT = []\n",
    "ORDER_BY_COUNT = []\n",
    "SELECT_COUNT =[]\n",
    "WHERE_COUNT = []\n",
    "TOTAL_CORRECT_COUNT = []\n",
    "\n",
    "\n",
    "MODEL_ID_LIST = []\n",
    "\n",
    "model_results_dir = os.listdir(\"../output/inference_/\")\n",
    "model_results_dir.remove(\"accumulated_accuracy\")\n",
    "for model_path in model_results_dir:\n",
    "\n",
    "\n",
    "    df_selected = pd.read_csv(base_path_inference+model_path)\n",
    "    grouped = df_selected.groupby('Expected classification_new')\n",
    "\n",
    "    execution_accuracy = {}\n",
    "\n",
    "    cat_df = pd.DataFrame(columns=[\"category\",\"Total evaluation records for category\",\"Total correct responses\",\"Execution accuracy %\"])\n",
    "    for name, group in grouped:\n",
    "        cat_list=[]\n",
    "        total_true = group['evalScorePostProcessing'].sum()\n",
    "        total_records = len(group)\n",
    "        #print(total_true,total_records,\"##\")\n",
    "        accuracy = total_true / total_records\n",
    "        execution_accuracy[name] = accuracy\n",
    "        cat_list.append(name)\n",
    "        cat_list.append(total_records)\n",
    "        cat_list.append(total_true)\n",
    "        cat_list.append(accuracy)\n",
    "        #print(cat_list)\n",
    "        new_row_df = pd.DataFrame([cat_list], columns=cat_df.columns)\n",
    "        cat_df = pd.concat([cat_df, new_row_df], ignore_index=True)\n",
    "\n",
    "    total_aggregate = cat_df[cat_df[\"category\"] ==  \"TOTAL\"]\n",
    "\n",
    "    total_evaluation_records = cat_df['Total evaluation records for category'].sum()\n",
    "    cat_df['Weights'] = cat_df['Total evaluation records for category'] / total_evaluation_records\n",
    "\n",
    "    tot_list = []\n",
    "    category = \"TOTAL\"\n",
    "    total_eval_records = df_selected.shape[0]\n",
    "    evalScore_counts = df_selected[df_selected['evalScorePostProcessing'] == True]['evalScorePostProcessing'].value_counts()[0]\n",
    "    exec_accuracy =  evalScore_counts/total_eval_records*100\n",
    "    tot_list.append(category)\n",
    "    tot_list.append(total_eval_records)\n",
    "    tot_list.append(evalScore_counts)\n",
    "    tot_list.append(exec_accuracy)\n",
    "    cat_df['Execution accuracy %'] = round(cat_df['Execution accuracy %']*100,2)\n",
    "    cat_df['Weighted execution accuracy'] = cat_df['Execution accuracy %'] * cat_df['Weights']\n",
    "\n",
    "    tot_list.append(np.nan)\n",
    "    tot_list.append(np.nan)\n",
    "    new_row_df = pd.DataFrame([tot_list], columns=cat_df.columns)\n",
    "    cat_df = pd.concat([cat_df, new_row_df], ignore_index=True)\n",
    "    cat_df['Average Latency'] = np.nan\n",
    "    cat_df['Average Latency'].iloc[-1] = round(df_selected['latency'].mean(),3)\n",
    "\n",
    "    model_results_dict = dict(zip(cat_df[\"category\"],cat_df['Execution accuracy %']))\n",
    "\n",
    "    model_count_dict = dict(zip(cat_df[\"category\"],cat_df['Total correct responses']))\n",
    "\n",
    "\n",
    "    AGGREGATE_RATIO.append(model_results_dict['AGGREGATE/RATIO'])\n",
    "    GROUP_BY.append(model_results_dict['GROUP BY'])\n",
    "    ORDER_BY.append(model_results_dict['ORDER BY'])\n",
    "    SELECT.append(model_results_dict['SELECT'])\n",
    "    WHERE.append(model_results_dict['WHERE'])\n",
    "    TOTAL_AVERAGE.append(model_results_dict['TOTAL'])\n",
    "\n",
    "\n",
    "\n",
    "    AGGREGATE_RATIO_COUNT.append(model_count_dict['AGGREGATE/RATIO'])\n",
    "    GROUP_BY_COUNT.append(model_count_dict['GROUP BY'])\n",
    "    ORDER_BY_COUNT.append(model_count_dict['ORDER BY'])\n",
    "    SELECT_COUNT.append(model_count_dict['SELECT'])\n",
    "    WHERE_COUNT.append(model_count_dict['WHERE'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    MODEL_ID_LIST.append(model_path.split(\".csv\")[0].replace(\"_exEvaluator\",\"\"))\n",
    "\n",
    "\n",
    "df_accuracy_query_type = pd.DataFrame({\"Model Id\":MODEL_ID_LIST,'SELECT': SELECT,'ORDER BY': ORDER_BY,'WHERE': WHERE, 'GROUP BY': GROUP_BY,\"AGGREGATE/RATIO\":AGGREGATE_RATIO,'TOTAL AVERAGE ACCURACY': TOTAL_AVERAGE,})\n",
    "\n",
    "df_count_query_type = pd.DataFrame({\"Model Id\":MODEL_ID_LIST,'SELECT': SELECT_COUNT,'ORDER BY': ORDER_BY_COUNT,'WHERE': WHERE_COUNT, 'GROUP BY': GROUP_BY_COUNT,\"AGGREGATE/RATIO\":AGGREGATE_RATIO_COUNT,})\n",
    "\n",
    "\n",
    "# tolal_list = [\"Total evaluation records for category\"]\n",
    "# tolal_list.extend(cat_df[\"Total evaluation records for category\"][:-1].to_list())\n",
    "# new_row_df = pd.DataFrame([tolal_list], columns=df_count_query_type.columns)\n",
    "# df_count_query_type = pd.concat([df_count_query_type, new_row_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645bde4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accuracy_query_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3309f423",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_query_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc14000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
