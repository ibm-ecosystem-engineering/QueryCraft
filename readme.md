# QueryCraft (by SuperKnowa)

Welcome to the `SuperKnowa-QueryCraft` repository, your comprehensive solution for fine-tuning Large Language Models (LLMs) for the task of generating SQL queries from natural language. This module is designed to streamline the process of adapting LLMs for Text2SQL tasks, providing a robust framework and pipeline that accelerates the initiation and fine-tuning process. Whether you're a developer or a user keen on harnessing the power of LLMs for database querying, `SuperKnowa-QueryCraft` offers the tools and infrastructure to make your journey smoother and more efficient.

You need the following to run the QueryCraft pipeline:
- Database to run SQL query on and to evaluate the generated queries
- Instruct set for fine tuning (you can curate instruct it with Annotation tool)
  
---
## Why to use?
- Use QueryCraft to query relational databases using natural language questions instead of writing complex SQL queries
- Use QueryCraft to build a configurable TextToSQL pipeline on your Database
- Use QueryCraft to to evaluate the Text To SQL pipeline on your Database
- Use QueryCraft to to correct the LLM-generated queries
- Use QueryCraft to to fine-tune an LLM for SQL query generation on your database
- Use QueryCraft to to speed up inferencing for SQL query generation
- Use QueryCraft to to analyze the generated queries on a dashboard

---
Benchmark Results 
- Add them here on various Spider across models
- <insightful > models working bettter on complex SQL
- Smaller fine-tuned models outperforming larger ones
- Pre and post processing improvement
- <>

<call pout code-granite as >

====

======

## Features/Modules in QueryCraft
The QueryCraft pipeline is built of 8 modules/components.

1. **Instruct dataset curation**: This is a UI tool to curate instruct dataset for finetuning or evaluation of TextToSQL task. Read details about this module on our blog on [Golden dataset curation for text2sql](https://medium.com/@himadricuet/138f2e77847e).

    <img src= "document/annotaion_tool.gif">

2. **Data ingestion**: Data ingestion module can be used for ingesting your data to Db2 on IBM Cloud.

    <img src= "image/Db2_ingetion.gif">

3. **Context Retriever**: Context retriever can be used to extract context from the instruct dataset given the instruct dataset. Read details about this module on our blog on [Diving Deep into Context Retrieval Techniques](https://medium.com/@sahil_desai/07b5f4b51273)

    <img src= "image/Context_ret.gif">


4. **Finetune**: Finetune service can be used for PEFT (Parameter Efficient Fine Tuning) for Text To SQL task. Read details about this module on our blog on [Text-to-SQL: A Deep Dive into Fine-Tuning Strategies](https://medium.com/@ak16425/59e2185b4b74)

    <img src= "image/fine_tune.gif">


5. **Inference**:
Inference service can be used to generate SQL queries either using a fine-tuned model or a pre-trained model on your NL-SQL pairs dataset. Read details about this module on our blog on [LLM inference text2sql](https://medium.com/@himadricuet/317a660de678)

    <img src= "image/Inference.gif">

6. **Query Correction**:
Inference service can be used to fix incorrect queries generated by LLM.


Read details about this module on our blog on [Improving Text-To-SQL Accuracy with Query Correction Service](https://medium.com/@abhilashamangal/614cd395a85b)


8. **Evaluation**:
Evaluation service consists of two modules - Query correction and Execution evaluation. Read details about our Evaluation framework on our blog [Text To SQL Evaluation Framework](https://medium.com/@Shivam_Solanki/8c5f461e7b05). Execution evaluation service can be used to calculate the execution accuracy on your database. Read details about our Evaluation framework on our blog [Text To SQL Execution Accuracy](https://medium.com/@Shivam_Solanki/5d7ab8398232)

    <img src= "image/evalution.gif">


9. **Query Analysis dashboard**
Query Analysis dashboard service can be used to analyze the results of generated queries and compare output of multiple LLMs. Read details about our Evaluation framework on our blog [Exploring LLM performance in Text-To-SQL generation with Query Analysis Dashboard](https://medium.com/@abhilashamangal/ccd547a020fd)

    <img src= "image/Dashboard.gif">

## Getting Started

To get started with `SuperKnowa-QueryCraft`, clone this repository to your local machine:

```bash
git clone https://github.com/ibm-ecosystem-engineering/SuperKnowa-QueryCraft.git
cd SuperKnowa-QueryCraft
```

### Installation

Before diving into fine-tuning or inference, ensure that your environment is set up with all the necessary dependencies:

```bash
pip install -r requirements.txt
```

### Configuration Files
- You have two options
- config-- If you want to do xyz use this 
- Expertconfig and if you want to specify your fine tuning use this and here is how to access 

Configure your environment and services by editing the  `superConfig.ini` and `config.ini` files. In superConfig specify the paths for datasets, models, and other services as per your setup. In `config.ini` you can change the more detailed parameters for experimentation.

`SuperKnowa-QueryCraft` provides the capability to run the whole pipeline (Context Retriever -> Fine-tuning -> Inference -> Query Correction -> Evaluation -> Query Analysis dashboard) together and also you can run each component individually. 

## Step by Step instructions
Please read the [detailed documentation](/document/readme.md) for step by step instructions

## Step 0. Instruct dataset

There are three options for using your dataset to finetune/evaluate the Text to SQL (QueryCraft) pipeline:

1. Bring your dataset with golden queries in the following format: question, query, and db_id. Instruction for ingesting the dataset is provided in the next Step 1.
1. Curate the golden query dataset using our annotation tool: <https://annotator.superknowa.tsglwatson.buildlab.cloud/>
1. Use the example datasets provided below for testing: Spider and KaggleDBQA

### Golden Query Annotation:
1. Go to our annotation tool. <https://annotator.superknowa.tsglwatson.buildlab.cloud/>

    ![Data annotator view](document/011.png)

2. Click on the Instruction Manual and follow the instructions for curating the golden queries dataset. <https://annotator.superknowa.tsglwatson.buildlab.cloud/documentation>

    ![Data annotation instruction manual](document/012.png)

## Step 1. Data Ingestion 
You have 3 options for Data Ingestion.
1. Bring Your Own Data
  - If you have both databases and instruct set (golden query)
  - If you only have database and not instruct set then use above annotation tool 
2. Use the example set
  - This comes with both source dataset and Instruct Db

Read the detailed steps for Data ingestion in [documentation](/document/readme.md)

Run the Data Ingestion module of the QueryCraft pipeline using the `runQueryCraft.sh`, file with the `dataIngestion` option after setting the `superConfig.ini` file to insert `salary.csv` into the `querycraft_db2_testing_13march` table in db2.  

```bash
sh runQueryCraft.sh
```
Provide the option:
```bash
dataIngestion
```

## Step 2. Context Retriever

Execute the context retriever using the following command. 

```bash
sh runQueryCraft.sh
```

Provide the option:

```bash
contextRetriever
```

## Step 3. Fine-Tuning

To start fine-tuning your LLM for the Text to SQL task, run the below command.

```bash
sh runQueryCraft.sh
```
Provide the option:
```bash
finetune
```

Follow the prompts to specify your dataset and model configuration.

## Step 4. Inference

To generate SQL queries using your fine-tuned or pre-trained model, execute:

```bash
sh runQueryCraft.sh
```
Provide the option:
```bash
inference
```

## Step 5. Query Correction

```bash
sh runQueryCraft.sh
```
Provide the option:
```bash
querycorrection
```

## Step 6. Evaluation

Evaluate the performance of your model against the SQLite database or DB2 by running the below command:

```bash
sh runQueryCraft.sh
```
Provide the option:
```bash
evaluation
```


## Step 7. Query Analysis Dashboard

For a visual analysis of your fine-tuning experiments and generated SQL queries, launch the streamlit dashboard:

```bash
sh runQueryCraft.sh
```
Provide the option:
```bash
queryanalysisDashboard
```

```bash
 cd code
 streamlit run streamlit_query_analysis_dashboard.py --server.port 8502 --server.fileWatcherType none
```


## Step 8. Run pipeline (all)
To run all components together, you can change the required parameters in `superConfig.ini`. You must set the default path as shown in the designated section below. 

```
[Finetune]
train_dataset =${Default:home_dir}input/datasets/${Default:exp_name}_contextRetriever.csv

[Inference]
input_dataset = input/datasets/${Default:exp_name}_validSet.csv


[QueryCorrection]
input_dataset = ${Default:home_dir}output/inference/${Default:exp_name}_inference.csv

[EXEvaluator]
input_dataset = ${Default:home_dir}output/inference/${Default:exp_name}_inference.csv
```

Run the below command:

```bash
sh runQueryCraft.sh
```

Provide the option:
```bash
all
```

## Build Lab Team 

Created & Architected By
- [Kunal Sawarkar](), Chief Data Scientist

Builders
- [Shivam Solanki](https://github.com/biharicoder)
- [Kevin Huang](https://github.com/kevinxhuang)
- [Abhilasha Mangal](https://github.com/Abhilasha-Mangal)
- [Amit Khandelwal](https://github.com/ak16425)
- [Sahil Desai](https://github.com/sahil11129)
- [Himadri Talukder](https://github.com/hitalukder)

## License

This project is licensed under the Apache-2.0 license - see the [LICENSE](LICENSE) file for details. Copyright @ 2024 IBM Corporation.
