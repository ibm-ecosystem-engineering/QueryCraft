# QueryCraft-The-SuperKnowa-SQL-Sculptor

Welcome to the `QueryCraft-The-SuperKnowa-SQL-Sculptor` repository, your comprehensive solution for fine-tuning Large Language Models (LLMs) for the task of generating SQL queries from natural language. This module is designed to streamline the process of adapting LLMs for Text to SQL tasks, providing a robust framework and pipeline that accelerates the initiation and fine-tuning process. Whether you're a developer or a user keen on harnessing the power of LLMs for database querying, `SuperKnowa SQL Sculptor` offers the tools and infrastructure to make your journey smoother and more efficient.

## Features

- **superConfig.ini**: A super configuration file to manage basic level settings for data ingestion, context retriever, fine-tuning, inference, query-correction, evaluation, and the query analysis dashboard among other services.
- **config.ini**: A configuration file to manage all settings for data ingestion, context retriever, fine-tuning, inference, query-correction, evaluation, and the query analysis dashboard among other services.
- **db2_connector.py**: This script provides global IBM DB2 connection for all other services.
- **db2_ingestion.py**: This script is used to insert the data into DB2 from a CSV file or any other delimiter file.
- **context_retriever.py**: This script extracts context directly from the SQLite database to inform and improve the accuracy of generated queries.
- **finetune.py**: Fine-tune your LLM specifically for the text-to-SQL task, optimizing its performance for your unique requirements.
- **inference.py**: Run the inference pipeline using either a pre-trained or a fine-tuned model to generate SQL queries from natural language inputs.
- **query_correction.py**: A script dedicated to correcting the syntax of generated SQL queries.
- **ex_evaluator.py**: A script dedicated to calculating the execution accuracy of generated SQL queries against an SQLite database, ensuring that your fine-tuned model performs optimally.
- **pipeline_result_csv_gen.py**: Extracts details of fine-tuning experiments and saves them to a CSV file, facilitating in-depth analysis through the query analysis dashboard.
- **streamlit_query_analysis_dashboard.py**: A Streamlit application that provides a comprehensive dashboard for analyzing the results of your fine-tuned model and conducting comparative analyses.

## Input

This module uses various inputs to train, evaluate, and optimize the performance of your model:

- **datasets/**: Contains datasets used for fine-tuning and evaluation of the Text to SQL model.
- **prompts/**: A collection of prompts used across different models for generation and fine-tuning tasks.
- **spider/**: The Spider database, serves as a benchmarking tool for evaluating the performance of your Text to SQL tasks.

## Output

The output directories are structured to organize the results of the different services and experiments:

- **benchmark/**: Stores benchmark results for Text to SQL tasks, allowing for performance comparison.
- **evalResults/**: A folder dedicated to storing the results of the evaluation service.
- **inference/**: Contains the results of the inference service, showcasing the SQL queries generated by your model.
- **logs/**: Logs of the experiments are kept here for future reference and analysis.
- **model/**: A directory to store model checkpoints, facilitating easy retrieval and deployment of your fine-tuned models.
- **result/**: A folder dedicated to storing the results of logs to show the results at the Query analysis dashboard.

## Getting Started

To get started with `QueryCraft-The-SuperKnowa-SQL-Sculptor`, clone this repository to your local machine:

```bash
git clone https://github.com/ibm-ecosystem-engineering/QueryCraft-The-SuperKnowa-SQL-Sculptor.git
cd QueryCraft-The-SuperKnowa-SQL-Sculptor
```

### Installation

Before diving into fine-tuning or inference, ensure that your environment is set up with all the necessary dependencies:

```bash
pip install -r requirements.txt
```

### Configuration

Configure your environment and services by editing the  `superConfig.ini` and `config.ini` files. In superConfig specify the paths for datasets, models, and other services as per your setup. In `config.ini` you can change the more detailed parameters for experimentation.

`QueryCraft-The-SuperKnowa-SQL-Sculptor` provides the capability to run the whole pipeline (Context Retriever -> Fine-tuning -> Inference -> Query Correction -> Evaluation -> Query Analysis dashboard) together and also you can run each component individually. 

### Run pipeline (all)
To run all components together, you can change the required parameters in `superConfig.ini`. and run the below command:

```bash
sh runQueryCraft.sh
```

Provide the option:
```bash
all
```

To run the individual component you can follow the below steps:

### 1. Data Ingestion (skip if source data is in sqllite database)
To start the data ingestion component you need to follow the below steps:
#### Prerequisites:
- Access to a DB2 database.
- The DB2_Ingestion module installed.
- Service credentials for the DB2 database.

#### Setup:
  Set the following credentials in the config.ini file under the [DB2_Credentials] section:
- ***dsn_database:*** Name of the database.
- ***dsn_uid:*** User ID for the database.
- ***dsn_pwd:*** Password for the database.
- ***dsn_hostname:*** Hostname or IP address of the database server.
- ***dsn_port:*** Port number of the database server.
- ***dsn_protocol:*** Protocol used for communication.
- ***dsn_driver:*** Driver used for database connection.

***Note:*** Get the DB2 credentials from <a href ='https://cloud.ibm.com/docs/Db2onCloud?topic=Db2onCloud-getting-started'>IBM cloud.</a>
 You can run the below command and start the data ingestion into the DB2.

```bash
sh runQueryCraft.sh
```
Provide the option:
```bash
dataIngestion
```
<img src= "image/Db2_ingetion.gif">
### 2. Context Retriever
To run the context retriever change the required parameters according to your db_type sqlite or DB2 into `config.ini` file and run the below command.

```bash
sh runQueryCraft.sh
```
Provide the option:
```bash
contextRetriever
```
<img src= "image/Context_ret.gif">
### 3. Fine-Tuning

To start fine-tuning your LLM for the Text to SQL task, run the below command.

```bash
sh runQueryCraft.sh
```
Provide the option:
```bash
finetune
```
<img src= "image/fine_tune.gif">

Follow the prompts to specify your dataset and model configuration.

### 4. Inference

To generate SQL queries using your fine-tuned or pre-trained model, execute:

```bash
sh runQueryCraft.sh
```
Provide the option:
```bash
inference
```
<img src= "image/Inference.gif">

### 5. Query Correction

```bash
sh runQueryCraft.sh
```
Provide the option:
```bash
querycorrection
```

### 6. Evaluation

Evaluate the performance of your model against the SQLite database or DB2 by running the below command:

```bash
sh runQueryCraft.sh
```
Provide the option:
```bash
evaluation
```

<img src= "image/evalution.gif">

## 7. Query Analysis Dashboard

For a visual analysis of your fine-tuning experiments and generated SQL queries, launch the streamlit dashboard:

```bash
sh runQueryCraft.sh
```
Provide the option:
```bash
queryanalysisDashboard
```

```bash
 streamlit run streamlit_query_analysis_dashboard.py --server.port 8052 --server.fileWatcherType none
```


<img src= "image/Dashboard.gif">

## JupyterLab Installation and Setup 

1. Install JupyterLab
To begin, install JupyterLab using pip, which is the Python package manager:

`pip install jupyterlab`

2. Launch JupyterLab
Once JupyterLab is installed, navigate to your project directory using the terminal or command prompt. Then, execute the following command to launch JupyterLab:

```jupyter lab ```

3. Accessing Jupyter Lab
You can access JupyterLab through your web browser by navigating to the following URL:

[JupyterLab Link](http://localhost:5005/lab)

Insert Notebook Password
Ex. Password: ibm@123

This will open JupyterLab interface in your browser, allowing you to create and manage Jupyter notebooks and other interactive documents.


## License

This project is licensed under the Apache-2.0 license - see the [LICENSE](LICENSE) file for details.
