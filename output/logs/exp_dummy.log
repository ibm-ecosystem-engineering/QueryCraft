INFO:root:EXPERIMENT :exp_dummy
INFO:root: Training Set : /data/rlhf/amit/TEST/SuperKnowa-TextToSQL/input/datasets/spiderTrainSetNewContext.csv
INFO:root: Base Model : codellama/CodeLlama-7b-hf
INFO:root: Finetuning Method : LORA
INFO:root: Precision : 8
INFO:root: Max length in tokenizer : 512
INFO:root: LoRA_r  : 64
INFO:root: LoRA_dropout  : 0.1
INFO:root: Batch Size  : 16
INFO:root: Number of train epochs  : 1
INFO:root: per_device_train_batch_size  : 1
INFO:root: Output Directory : /data/rlhf/amit/TEST/SuperKnowa-TextToSQL/output/model/exp_dummy
INFO:root: Target Modules: attention_linear_layers
INFO:root: Number of samples for training: 7000
INFO:root: Number of samples for validation: 700
WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:root:Time taken to run in seconds: :225.80719637870789
INFO:root:EXPERIMENT :exp_dummy
INFO:root: Training Set : /data/rlhf/amit/TEST/SuperKnowa-TextToSQL/input/datasets/spiderTrainSetNewContext.csv
INFO:root: Base Model : codellama/CodeLlama-7b-hf
INFO:root: Finetuning Method : LORA
INFO:root: Precision : 8
INFO:root: Max length in tokenizer : 512
INFO:root: LoRA_r  : 64
INFO:root: LoRA_dropout  : 0.1
INFO:root: Batch Size  : 16
INFO:root: Number of train epochs  : 1
INFO:root: per_device_train_batch_size  : 1
INFO:root: Output Directory : /data/rlhf/amit/TEST/SuperKnowa-TextToSQL/output/model/exp_dummy
INFO:root: Target Modules: attention_linear_layers
INFO:root: Number of samples for training: 7000
INFO:root: Number of samples for validation: 700
WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
